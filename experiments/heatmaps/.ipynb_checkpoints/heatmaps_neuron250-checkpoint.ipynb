{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "288d24de",
   "metadata": {},
   "source": [
    "# Heatmaps for Neuron 250 (real corpus)\n",
    "This notebook loads `best_lstm_model.pt` from the project folder, rebuilds the vocabulary from `reconstructed_training_corpus.txt`, and produces heatmaps of $c_{t,250}$ on real strings from `reconstructed_Lstm_corpus.txt` (filtered to contain parentheses).\n",
    "\n",
    "It also generates lesion and clamp variants (inference-time interventions on the cell state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bee7727",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Imports & device ---\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49c781c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Paths (assumes this notebook is in the same folder as the project files) ---\n",
    "PROJECT_DIR = Path(\".\").resolve()\n",
    "\n",
    "MODEL_PATH = PROJECT_DIR / \"best_lstm_model.pt\"\n",
    "TRAIN_CORPUS_PATH = PROJECT_DIR / \"reconstructed_training_corpus.txt\"\n",
    "EVAL_CORPUS_PATH  = PROJECT_DIR / \"reconstructed_Lstm_corpus.txt\"   # used for \"real test strings\" visualization\n",
    "\n",
    "assert MODEL_PATH.exists(), f\"Missing: {MODEL_PATH}\"\n",
    "assert TRAIN_CORPUS_PATH.exists(), f\"Missing: {TRAIN_CORPUS_PATH}\"\n",
    "assert EVAL_CORPUS_PATH.exists(), f\"Missing: {EVAL_CORPUS_PATH}\"\n",
    "\n",
    "print(\"Project dir:\", PROJECT_DIR)\n",
    "print(\"Model:\", MODEL_PATH)\n",
    "print(\"Train corpus:\", TRAIN_CORPUS_PATH)\n",
    "print(\"Eval corpus:\", EVAL_CORPUS_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e3fc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Config copied from the training notebook (R^2_Score_0_6971) ---\n",
    "CONFIG = {\n",
    "    \"max_pair_dist\": 280, \"max_depth\": 2, \"n_paragraphs\": 3000, \"min_char_freq\": 5,\n",
    "    \"lowercase\": False, \"fold_diacritics\": True, \"digit_map\": \"none\", \"collapse_whitespace\": True,\n",
    "    \"hidden_size\": 512, \"embedding_dim\": 128, \"n_layers\": 1,\n",
    "    \"dropout\": 0.17723111503731978, \"forget_bias\": 0.673403567309223,\n",
    "    \"seq_length\": 256, \"batch_size\": 128, \"lr\": 0.0008848537716447663,\n",
    "    \"weight_decay\": 2.5444743948884783e-06, \"grad_clip\": 5.0, \"epochs\": 40,\n",
    "    \"probe_alpha\": 10.0, \"probe_subsample_every\": 1, \"probe_max_paras_per_lang\": 200,\n",
    "    \"probe_max_len\": 400, \"probe_exclude_parens\": True, \"probe_test_size\": 0.2, \"seed\": 10,\n",
    "}\n",
    "\n",
    "UNK_CHAR = \"\\u0000\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee367fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Vocabulary rebuild (must match training procedure to be compatible with the saved state_dict) ---\n",
    "def build_vocab_from_text(text: str, *, min_char_freq: int):\n",
    "    freq = {}\n",
    "    for ch in text:\n",
    "        freq[ch] = freq.get(ch, 0) + 1\n",
    "\n",
    "    # ensure these exist\n",
    "    for ch in (UNK_CHAR, \"(\", \")\", \"\\n\", \" \"):\n",
    "        freq.setdefault(ch, 10**9)\n",
    "\n",
    "    chars = sorted(set([ch for ch, c in freq.items() if c >= min_char_freq] + [UNK_CHAR, \"(\", \")\"]))\n",
    "    char2idx = {ch: i for i, ch in enumerate(chars)}\n",
    "    idx2char = {i: ch for ch, i in char2idx.items()}\n",
    "    return char2idx, idx2char\n",
    "\n",
    "train_text = TRAIN_CORPUS_PATH.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "char2idx, idx2char = build_vocab_from_text(train_text, min_char_freq=CONFIG[\"min_char_freq\"])\n",
    "\n",
    "print(\"Vocab size:\", len(char2idx))\n",
    "print(\"First 20 chars:\", list(char2idx.keys())[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578d9616",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Model (same as in training notebook) ---\n",
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, n_layers, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim, hidden_size, n_layers,\n",
    "            dropout=dropout if n_layers > 1 else 0.0,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        h0 = torch.zeros(self.n_layers, batch_size, self.hidden_size, device=device)\n",
    "        c0 = torch.zeros(self.n_layers, batch_size, self.hidden_size, device=device)\n",
    "        return h0, c0\n",
    "\n",
    "    def forward(self, x, hc):\n",
    "        out, (h, c) = self.lstm(self.embedding(x), hc)\n",
    "        return self.fc(self.drop(out)), (h, c)\n",
    "\n",
    "def init_forget_gate_bias(lstm, forget_bias):\n",
    "    with torch.no_grad():\n",
    "        for layer in range(lstm.num_layers):\n",
    "            for name in (f\"bias_ih_l{layer}\", f\"bias_hh_l{layer}\"):\n",
    "                b = getattr(lstm, name)\n",
    "                hidden = lstm.hidden_size\n",
    "                # gate order: i, f, g, o\n",
    "                b[hidden:2*hidden].fill_(forget_bias)\n",
    "\n",
    "model = CharLSTM(\n",
    "    vocab_size=len(char2idx),\n",
    "    embedding_dim=CONFIG[\"embedding_dim\"],\n",
    "    hidden_size=CONFIG[\"hidden_size\"],\n",
    "    n_layers=CONFIG[\"n_layers\"],\n",
    "    dropout=CONFIG[\"dropout\"],\n",
    ").to(device)\n",
    "\n",
    "init_forget_gate_bias(model.lstm, CONFIG[\"forget_bias\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6d2d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Load weights ---\n",
    "obj = torch.load(MODEL_PATH, map_location=device)\n",
    "\n",
    "# Support both formats:\n",
    "# 1) plain state_dict (OrderedDict)\n",
    "# 2) checkpoint dict with key \"model_state_dict\" (as in the training notebook)\n",
    "if isinstance(obj, dict) and \"model_state_dict\" in obj:\n",
    "    state = obj[\"model_state_dict\"]\n",
    "    if \"char2int\" in obj and obj[\"char2int\"] is not None:\n",
    "        # if available, use the saved mapping (best)\n",
    "        saved = obj[\"char2int\"]\n",
    "        # training notebook used char2int naming; adapt to char2idx\n",
    "        char2idx = saved\n",
    "        idx2char = {i: ch for ch, i in char2idx.items()}\n",
    "        # rebuild model with correct vocab size if needed\n",
    "        if model.embedding.num_embeddings != len(char2idx):\n",
    "            model = CharLSTM(\n",
    "                vocab_size=len(char2idx),\n",
    "                embedding_dim=CONFIG[\"embedding_dim\"],\n",
    "                hidden_size=CONFIG[\"hidden_size\"],\n",
    "                n_layers=CONFIG[\"n_layers\"],\n",
    "                dropout=CONFIG[\"dropout\"],\n",
    "            ).to(device)\n",
    "            init_forget_gate_bias(model.lstm, CONFIG[\"forget_bias\"])\n",
    "else:\n",
    "    state = obj  # OrderedDict\n",
    "\n",
    "missing, unexpected = model.load_state_dict(state, strict=True)\n",
    "print(\"Loaded. missing:\", missing, \"unexpected:\", unexpected)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481a1e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Utilities: encoding + inside/outside labels ---\n",
    "def encode_string(s, char2idx):\n",
    "    unk = char2idx.get(UNK_CHAR, None)\n",
    "    if unk is None:\n",
    "        # fallback: drop unknowns\n",
    "        return torch.tensor([char2idx[c] for c in s if c in char2idx], dtype=torch.long)\n",
    "    return torch.tensor([char2idx.get(c, unk) for c in s], dtype=torch.long)\n",
    "\n",
    "def inside_parentheses_labels(s: str, open_char='(', close_char=')'):\n",
    "    depth = 0\n",
    "    y = []\n",
    "    for ch in s:\n",
    "        if ch == open_char:\n",
    "            y.append(0); depth += 1\n",
    "        elif ch == close_char:\n",
    "            depth = max(0, depth - 1); y.append(0)\n",
    "        else:\n",
    "            y.append(1 if depth > 0 else 0)\n",
    "    return np.array(y, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52c612d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Load real strings from eval corpus and sample ---\n",
    "lines = EVAL_CORPUS_PATH.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines()\n",
    "lines = [s.strip() for s in lines if s.strip()]\n",
    "\n",
    "# keep lines that contain parentheses\n",
    "strings = [s for s in lines if \"(\" in s and \")\" in s]\n",
    "print(\"Total lines:\", len(lines))\n",
    "print(\"With parentheses:\", len(strings))\n",
    "print(\"Example:\", strings[0][:160] if strings else \"NONE\")\n",
    "\n",
    "SEED = 0\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "N = 80\n",
    "sample = random.sample(strings, min(N, len(strings)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058e1fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Interventions (inference-time) ---\n",
    "BEST_NEURON = 250  # change if needed\n",
    "\n",
    "def lesion_intervention_factory(idx):\n",
    "    def _intervene(c, t):\n",
    "        c = c.clone()\n",
    "        c[-1, :, idx] = 0.0\n",
    "        return c\n",
    "    return _intervene\n",
    "\n",
    "def clamp_intervention_factory(idx, value):\n",
    "    def _intervene(c, t):\n",
    "        c = c.clone()\n",
    "        c[-1, :, idx] = float(value)\n",
    "        return c\n",
    "    return _intervene\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf93c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Build heatmap matrices for c_{t,250} ---\n",
    "MAX_T = 140\n",
    "\n",
    "def c_trace(s, intervention=None):\n",
    "    # keep chars in vocab; also truncate\n",
    "    s2 = \"\".join([ch for ch in s if ch in char2idx])[:MAX_T]\n",
    "    if len(s2) == 0:\n",
    "        return None, None, None\n",
    "\n",
    "    x = encode_string(s2, char2idx).unsqueeze(0).to(device)\n",
    "    emb = model.embedding(x)\n",
    "\n",
    "    hx = torch.zeros(model.n_layers, 1, model.hidden_size, device=device)\n",
    "    cx = torch.zeros(model.n_layers, 1, model.hidden_size, device=device)\n",
    "\n",
    "    vals = []\n",
    "    for t in range(emb.size(1)):\n",
    "        _, (hx, cx) = model.lstm(emb[:, t:t+1, :], (hx, cx))\n",
    "        if intervention is not None:\n",
    "            cx = intervention(cx, t)\n",
    "        vals.append(cx[-1, 0, BEST_NEURON].detach().float().cpu().item())\n",
    "\n",
    "    mask = inside_parentheses_labels(s2)\n",
    "    return np.array(vals, dtype=np.float32), mask, s2\n",
    "\n",
    "def build_heatmap(strings, intervention=None):\n",
    "    traces, masks = [], []\n",
    "    for s in strings:\n",
    "        tr, m, _ = c_trace(s, intervention=intervention)\n",
    "        if tr is None:\n",
    "            continue\n",
    "        traces.append(tr); masks.append(m)\n",
    "    if not traces:\n",
    "        raise ValueError(\"No valid strings for heatmap (check vocab / corpus).\")\n",
    "\n",
    "    T = min(max(len(tr) for tr in traces), MAX_T)\n",
    "    H = np.full((len(traces), T), np.nan, dtype=np.float32)\n",
    "    M = np.full((len(traces), T), np.nan, dtype=np.float32)\n",
    "\n",
    "    for i, (tr, m) in enumerate(zip(traces, masks)):\n",
    "        L = min(len(tr), T)\n",
    "        H[i, :L] = tr[:L]\n",
    "        M[i, :L] = m[:L]\n",
    "    return H, M\n",
    "\n",
    "def plot_heatmap(H, M, title, outpath=None):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(H, aspect=\"auto\", interpolation=\"nearest\")\n",
    "    plt.colorbar(label=rf\"$c_{{t,{BEST_NEURON}}}$\")\n",
    "    ys, xs = np.where(M == 1)\n",
    "    plt.scatter(xs, ys, s=2)  # inside points\n",
    "    plt.xlabel(\"t (character position)\")\n",
    "    plt.ylabel(\"example (real corpus line)\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    if outpath is not None:\n",
    "        plt.savefig(outpath, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "# Baseline\n",
    "H_base, M_base = build_heatmap(sample, intervention=None)\n",
    "plot_heatmap(H_base, M_base, f\"Baseline heatmap: $c_{{t,{BEST_NEURON}}}$ (points = inside) \", outpath=\"heatmap_baseline.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a28971",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Lesion heatmap ---\n",
    "H_les, M_les = build_heatmap(sample, intervention=lesion_intervention_factory(BEST_NEURON))\n",
    "plot_heatmap(H_les, M_les, f\"Lesion heatmap: force $c_{{t,{BEST_NEURON}}}=0$\", outpath=\"heatmap_lesion.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afe75db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Estimate K from real strings (mean inside value) and clamp heatmaps ---\n",
    "def estimate_K(strings, max_samples=50):\n",
    "    vals = []\n",
    "    for s in strings[:max_samples]:\n",
    "        tr, m, _ = c_trace(s, intervention=None)\n",
    "        if tr is None:\n",
    "            continue\n",
    "        inside = tr[m == 1]\n",
    "        if inside.size > 0:\n",
    "            vals.append(float(np.mean(inside)))\n",
    "    return float(np.mean(vals)) if vals else 1.5\n",
    "\n",
    "K_real = estimate_K(sample)\n",
    "print(\"Estimated K (real corpus) =\", K_real)\n",
    "\n",
    "H_c0, M_c0 = build_heatmap(sample, intervention=clamp_intervention_factory(BEST_NEURON, 0.0))\n",
    "plot_heatmap(H_c0, M_c0, f\"Clamp heatmap: force $c_{{t,{BEST_NEURON}}}=0$\", outpath=\"heatmap_clamp0.png\")\n",
    "\n",
    "H_cK, M_cK = build_heatmap(sample, intervention=clamp_intervention_factory(BEST_NEURON, K_real))\n",
    "plot_heatmap(H_cK, M_cK, f\"Clamp heatmap: force $c_{{t,{BEST_NEURON}}}=K$ (Kâ‰ˆ{K_real:.2f})\", outpath=\"heatmap_clampK.png\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
