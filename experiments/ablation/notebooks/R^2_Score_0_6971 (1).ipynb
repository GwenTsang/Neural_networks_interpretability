{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch in /venv/main/lib/python3.10/site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /venv/main/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: filelock in /venv/main/lib/python3.10/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: fsspec in /venv/main/lib/python3.10/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /venv/main/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: jinja2 in /venv/main/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: networkx in /venv/main/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /venv/main/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /venv/main/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /venv/main/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /venv/main/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: triton==3.1.0 in /venv/main/lib/python3.10/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /venv/main/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: sympy==1.13.1 in /venv/main/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /venv/main/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /venv/main/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /venv/main/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /venv/main/lib/python3.10/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /venv/main/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /venv/main/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /venv/main/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /venv/main/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install torch --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting scipy>=1.8.0\n",
      "  Downloading scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting joblib>=1.2.0\n",
      "  Downloading joblib-1.5.3-py3-none-any.whl (309 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.1/309.1 KB\u001b[0m \u001b[31m133.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.22.0 in /venv/main/lib/python3.10/site-packages (from scikit-learn) (2.1.3)\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.5.3 scikit-learn-1.7.2 scipy-1.15.3 threadpoolctl-3.6.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Ty66jb8PnJjU"
   },
   "outputs": [],
   "source": [
    "!wget -q https://raw.githubusercontent.com/GwenTsang/tests/refs/heads/main/all_Flaubert.txt\n",
    "!wget -q https://raw.githubusercontent.com/GwenTsang/tests/refs/heads/main/french.txt\n",
    "!wget -q https://raw.githubusercontent.com/GwenTsang/tests/refs/heads/main/l.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8aAZTIBenFx-",
    "outputId": "ecdcc5a8-40f5-46db-c5d2-8a1241bd8f0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Loaded 23644 paragraphs\n",
      "Training text: 3,640,437 chars\n",
      "Vocab size: 115\n",
      "Parameters: 1,388,531\n",
      "Epoch 1/40 - Loss: 2.5326\n",
      "Epoch 2/40 - Loss: 1.9547\n",
      "Epoch 3/40 - Loss: 1.7454\n",
      "Epoch 4/40 - Loss: 1.6105\n",
      "Epoch 5/40 - Loss: 1.5205\n",
      "Epoch 6/40 - Loss: 1.4586\n",
      "Epoch 7/40 - Loss: 1.4130\n",
      "Epoch 8/40 - Loss: 1.3772\n",
      "Epoch 9/40 - Loss: 1.3479\n",
      "Epoch 10/40 - Loss: 1.3229\n",
      "Epoch 11/40 - Loss: 1.3014\n",
      "Epoch 12/40 - Loss: 1.2814\n",
      "Epoch 13/40 - Loss: 1.2640\n",
      "Epoch 14/40 - Loss: 1.2479\n",
      "Epoch 15/40 - Loss: 1.2339\n",
      "Epoch 16/40 - Loss: 1.2208\n",
      "Epoch 17/40 - Loss: 1.2087\n",
      "Epoch 18/40 - Loss: 1.1979\n",
      "Epoch 19/40 - Loss: 1.1877\n",
      "Epoch 20/40 - Loss: 1.1782\n",
      "Epoch 21/40 - Loss: 1.1691\n",
      "Epoch 22/40 - Loss: 1.1611\n",
      "Epoch 23/40 - Loss: 1.1536\n",
      "Epoch 24/40 - Loss: 1.1468\n",
      "Epoch 25/40 - Loss: 1.1400\n",
      "Epoch 26/40 - Loss: 1.1335\n",
      "Epoch 27/40 - Loss: 1.1272\n",
      "Epoch 28/40 - Loss: 1.1219\n",
      "Epoch 29/40 - Loss: 1.1164\n",
      "Epoch 30/40 - Loss: 1.1112\n",
      "Epoch 31/40 - Loss: 1.1062\n",
      "Epoch 32/40 - Loss: 1.1019\n",
      "Epoch 33/40 - Loss: 1.0973\n",
      "Epoch 34/40 - Loss: 1.0927\n",
      "Epoch 35/40 - Loss: 1.0881\n",
      "Epoch 36/40 - Loss: 1.0842\n",
      "Epoch 37/40 - Loss: 1.0806\n",
      "Epoch 38/40 - Loss: 1.0768\n",
      "Epoch 39/40 - Loss: 1.0736\n",
      "Epoch 40/40 - Loss: 1.0702\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "#@title Setup & Training\n",
    "\n",
    "import os\n",
    "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Sequence\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "CONFIG = {\n",
    "    \"max_pair_dist\": 280, \"max_depth\": 2, \"n_paragraphs\": 3000, \"min_char_freq\": 5,\n",
    "    \"lowercase\": False, \"fold_diacritics\": True, \"digit_map\": \"none\", \"collapse_whitespace\": True,\n",
    "    \"hidden_size\": 512, \"embedding_dim\": 128, \"n_layers\": 1,\n",
    "    \"dropout\": 0.17723111503731978, \"forget_bias\": 0.673403567309223,\n",
    "    \"seq_length\": 256, \"batch_size\": 128, \"lr\": 0.0008848537716447663,\n",
    "    \"weight_decay\": 2.5444743948884783e-06, \"grad_clip\": 5.0, \"epochs\": 40,\n",
    "    \"probe_alpha\": 10.0, \"probe_subsample_every\": 1, \"probe_max_paras_per_lang\": 200,\n",
    "    \"probe_max_len\": 400, \"probe_exclude_parens\": True, \"probe_test_size\": 0.2, \"seed\": 10,\n",
    "}\n",
    "TRAIN_FILES = [\"all_Flaubert.txt\", \"french.txt\", \"l.txt\"]\n",
    "EVAL_FR_PATH, EVAL_EN_PATH = \"all_Flaubert.txt\", \"french.txt\"\n",
    "UNK_CHAR = \"\\u0000\"\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ParagraphInfo:\n",
    "    text: str; has_pair: bool; balanced: bool; max_pair_dist: int; max_depth: int\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TextNormConfig:\n",
    "    lowercase: bool; fold_diacritics: bool; digit_map: str; collapse_whitespace: bool\n",
    "\n",
    "def _analyze_parentheses(par: str) -> Tuple[bool, bool, int, int]:\n",
    "    has_pair = \"(\" in par and \")\" in par\n",
    "    stack, max_dist, depth, max_depth, balanced = [], 0, 0, 0, True\n",
    "    for i, ch in enumerate(par):\n",
    "        if ch == \"(\":\n",
    "            stack.append(i); depth += 1; max_depth = max(max_depth, depth)\n",
    "        elif ch == \")\":\n",
    "            if not stack: balanced = False; break\n",
    "            max_dist = max(max_dist, i - stack.pop()); depth = max(0, depth - 1)\n",
    "    if stack: balanced = False\n",
    "    return has_pair, balanced, max_dist, max_depth\n",
    "\n",
    "def load_paragraphs(files: Sequence[Path]) -> List[str]:\n",
    "    paragraphs = []\n",
    "    for fp in files:\n",
    "        txt = fp.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        paragraphs.extend([p.strip() for p in re.split(r\"\\n\\s*\\n\", txt) if p.strip()])\n",
    "    return paragraphs\n",
    "\n",
    "def build_paragraph_infos(files: Sequence[Path]) -> List[ParagraphInfo]:\n",
    "    return [ParagraphInfo(p, *_analyze_parentheses(p)) for p in load_paragraphs(files)]\n",
    "\n",
    "def normalize_text(s: str, cfg: TextNormConfig) -> str:\n",
    "    if cfg.lowercase: s = s.lower()\n",
    "    if cfg.fold_diacritics:\n",
    "        s = unicodedata.normalize(\"NFKD\", s)\n",
    "        s = \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
    "    if cfg.digit_map != \"none\": s = re.sub(r\"\\d\", \"#\" if cfg.digit_map == \"hash\" else \"0\", s)\n",
    "    if cfg.collapse_whitespace: s = re.sub(r\"[ \\t]+\", \" \", s); s = re.sub(r\"\\n{3,}\", \"\\n\\n\", s)\n",
    "    return s\n",
    "\n",
    "def make_training_text(infos, *, max_pair_dist, max_depth, n_paragraphs, seed, norm):\n",
    "    candidates = [pi.text for pi in infos if pi.has_pair and pi.balanced and pi.max_pair_dist <= max_pair_dist and pi.max_depth <= max_depth]\n",
    "    if not candidates: return \"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    chosen = [candidates[i] for i in rng.choice(len(candidates), size=min(n_paragraphs, len(candidates)), replace=False)]\n",
    "    return \"\\n\\n\".join([normalize_text(p, norm) for p in chosen])\n",
    "\n",
    "def build_vocab_and_encode(text: str, *, min_char_freq: int):\n",
    "    freq = {}\n",
    "    for ch in text: freq[ch] = freq.get(ch, 0) + 1\n",
    "    for ch in (UNK_CHAR, \"(\", \")\", \"\\n\", \" \"): freq.setdefault(ch, 10**9)\n",
    "    chars = sorted(set([ch for ch, c in freq.items() if c >= min_char_freq] + [UNK_CHAR, \"(\", \")\"]))\n",
    "    char2int = {ch: i for i, ch in enumerate(chars)}\n",
    "    unk = char2int[UNK_CHAR]\n",
    "    return char2int, np.fromiter((char2int.get(ch, unk) for ch in text), dtype=np.int64)\n",
    "\n",
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.vocab_size, self.hidden_size, self.n_layers = vocab_size, hidden_size, n_layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, n_layers, dropout=dropout if n_layers > 1 else 0.0, batch_first=True)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        return (torch.zeros(self.n_layers, batch_size, self.hidden_size, device=device),\n",
    "                torch.zeros(self.n_layers, batch_size, self.hidden_size, device=device))\n",
    "\n",
    "    def forward(self, x, hc):\n",
    "        out, (h, c) = self.lstm(self.embedding(x), hc)\n",
    "        return self.fc(self.drop(out)), (h, c)\n",
    "\n",
    "def init_forget_gate_bias(lstm, forget_bias):\n",
    "    with torch.no_grad():\n",
    "        for layer in range(lstm.num_layers):\n",
    "            for name in (f\"bias_ih_l{layer}\", f\"bias_hh_l{layer}\"):\n",
    "                getattr(lstm, name)[lstm.hidden_size:2*lstm.hidden_size].fill_(forget_bias)\n",
    "\n",
    "torch.manual_seed(CONFIG[\"seed\"]); np.random.seed(CONFIG[\"seed\"])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "norm = TextNormConfig(CONFIG[\"lowercase\"], CONFIG[\"fold_diacritics\"], CONFIG[\"digit_map\"], CONFIG[\"collapse_whitespace\"])\n",
    "infos = build_paragraph_infos([Path(f) for f in TRAIN_FILES])\n",
    "print(f\"Loaded {len(infos)} paragraphs\")\n",
    "\n",
    "text = make_training_text(infos, max_pair_dist=CONFIG[\"max_pair_dist\"], max_depth=CONFIG[\"max_depth\"],\n",
    "                          n_paragraphs=CONFIG[\"n_paragraphs\"], seed=CONFIG[\"seed\"], norm=norm)\n",
    "print(f\"Training text: {len(text):,} chars\")\n",
    "\n",
    "char2int, encoded = build_vocab_and_encode(text, min_char_freq=CONFIG[\"min_char_freq\"])\n",
    "vocab_size = len(char2int)\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "\n",
    "encoded = torch.tensor(encoded, dtype=torch.long, device=device)\n",
    "n_tokens = encoded.numel()\n",
    "\n",
    "model = CharLSTM(vocab_size, CONFIG[\"embedding_dim\"], CONFIG[\"hidden_size\"], CONFIG[\"n_layers\"], CONFIG[\"dropout\"]).to(device)\n",
    "init_forget_gate_bias(model.lstm, CONFIG[\"forget_bias\"])\n",
    "model.lstm.flatten_parameters()\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG[\"lr\"], weight_decay=CONFIG[\"weight_decay\"])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scaler = torch.amp.GradScaler(\"cuda\", enabled=device.type == \"cuda\")\n",
    "B, T = CONFIG[\"batch_size\"], CONFIG[\"seq_length\"]\n",
    "rng = torch.Generator(device=device); rng.manual_seed(CONFIG[\"seed\"])\n",
    "\n",
    "model.train()\n",
    "for epoch in range(CONFIG[\"epochs\"]):\n",
    "    off = int(torch.randint(0, T, (1,), generator=rng, device=device).item())\n",
    "    n_batches = (n_tokens - off - 1) // (B * T)\n",
    "    if n_batches < 1: continue\n",
    "    data = encoded[off: off + n_batches * B * T + 1]\n",
    "    mat, mat_y = data[:-1].reshape(B, -1), data[1:].reshape(B, -1)\n",
    "    hc = model.init_hidden(B, device)\n",
    "    epoch_loss = 0.0\n",
    "    for step in range(mat.size(1) // T):\n",
    "        x, y = mat[:, step*T:(step+1)*T], mat_y[:, step*T:(step+1)*T]\n",
    "        hc = (hc[0].detach(), hc[1].detach())\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.amp.autocast(\"cuda\", dtype=torch.float16, enabled=device.type == \"cuda\"):\n",
    "            logits, hc = model(x, hc)\n",
    "            loss = criterion(logits.reshape(-1, vocab_size), y.reshape(-1))\n",
    "        scaler.scale(loss).backward()\n",
    "        if CONFIG[\"grad_clip\"] > 0: scaler.unscale_(optimizer); nn.utils.clip_grad_norm_(model.parameters(), CONFIG[\"grad_clip\"])\n",
    "        scaler.step(optimizer); scaler.update()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{CONFIG['epochs']} - Loss: {epoch_loss / (mat.size(1) // T):.4f}\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GINNM-lun8hL",
    "outputId": "e11179ac-2d18-4132-d65d-9869b24ecde4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building probe dataset...\n",
      "Probe: 57448 train, 14362 test\n",
      "\n",
      "==================================================\n",
      "RESULTS\n",
      "==================================================\n",
      "Best R² (local): 0.7363\n",
      "Best neuron index: 250\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "#@title Evaluation\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def inside_after_reading(s: str) -> np.ndarray:\n",
    "    d, y = 0, np.zeros(len(s), dtype=np.int64)\n",
    "    for t, ch in enumerate(s):\n",
    "        if ch == \"(\": d += 1\n",
    "        elif ch == \")\": d = max(0, d - 1)\n",
    "        y[t] = 1 if d > 0 else 0\n",
    "    return y\n",
    "\n",
    "@torch.no_grad()\n",
    "def cell_states_last_layer(model, s, char2int, device, max_len):\n",
    "    if len(s) > max_len: s = s[:max_len]\n",
    "    unk = char2int[UNK_CHAR]\n",
    "    x = torch.tensor([char2int.get(ch, unk) for ch in s], device=device, dtype=torch.long).unsqueeze(0)\n",
    "    h, c = model.init_hidden(1, device)\n",
    "    model.lstm.flatten_parameters()\n",
    "    Cs = []\n",
    "    for t in range(x.size(1)):\n",
    "        _, (h, c) = model.lstm(model.embedding(x[:, t:t+1]), (h, c))\n",
    "        Cs.append(c[-1, 0].float().cpu().numpy())\n",
    "    return np.stack(Cs, axis=0).astype(np.float32)\n",
    "\n",
    "def build_probe_dataset(model, char2int, device, fr_path, en_path, cfg, norm):\n",
    "    rng = np.random.default_rng(cfg[\"seed\"])\n",
    "    model.eval()\n",
    "    def get_paras(path):\n",
    "        txt = path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        ps = [p.strip() for p in re.split(r\"\\n\\s*\\n\", txt) if p.strip() and \"(\" in p and \")\" in p]\n",
    "        if len(ps) > cfg[\"probe_max_paras_per_lang\"]:\n",
    "            ps = [ps[i] for i in rng.choice(len(ps), size=cfg[\"probe_max_paras_per_lang\"], replace=False)]\n",
    "        return ps\n",
    "    Xs, ys, langs = [], [], []\n",
    "    for lang, path in [(\"fr\", fr_path), (\"en\", en_path)]:\n",
    "        for p in get_paras(path):\n",
    "            p = normalize_text(p, norm)\n",
    "            if len(p) > cfg[\"probe_max_len\"]:\n",
    "                start = int(rng.integers(0, len(p) - cfg[\"probe_max_len\"] + 1))\n",
    "                p = p[start:start + cfg[\"probe_max_len\"]]\n",
    "            y = inside_after_reading(p)\n",
    "            X = cell_states_last_layer(model, p, char2int, device, cfg[\"probe_max_len\"])\n",
    "            mask = np.ones(len(p), dtype=bool)\n",
    "            if cfg[\"probe_exclude_parens\"]: mask &= np.array([c not in \"()\" for c in p])\n",
    "            if cfg[\"probe_subsample_every\"] > 1:\n",
    "                take = np.zeros(len(p), dtype=bool); take[::cfg[\"probe_subsample_every\"]] = True; mask &= take\n",
    "            if mask.sum() >= 10:\n",
    "                Xs.append(X[mask]); ys.append(y[mask]); langs.append(np.full(mask.sum(), lang, dtype=object))\n",
    "    X, y = np.concatenate(Xs), np.concatenate(ys).astype(np.float32)\n",
    "    return train_test_split(X, y, test_size=cfg[\"probe_test_size\"], random_state=cfg[\"seed\"], stratify=np.concatenate(langs))\n",
    "\n",
    "def ridge_r2_local(X_train, y_train, X_test, y_test, alpha):\n",
    "    Xtr, Xte = X_train.astype(np.float64), X_test.astype(np.float64)\n",
    "    ytr, yte = y_train.astype(np.float64), y_test.astype(np.float64)\n",
    "    x_mean, y_mean = Xtr.mean(0), ytr.mean()\n",
    "    Xtr_c, ytr_c = Xtr - x_mean, ytr - y_mean\n",
    "    w = (Xtr_c.T @ ytr_c) / (np.sum(Xtr_c**2, 0) + alpha)\n",
    "    Xte_c, a = Xte - x_mean, yte - y_mean\n",
    "    sse = np.sum(a**2) - 2*(w * (Xte_c.T @ a)) + (w**2) * np.sum(Xte_c**2, 0)\n",
    "    ss_tot = np.sum((yte - yte.mean())**2)\n",
    "    r2 = 1 - sse / ss_tot if ss_tot > 1e-12 else np.zeros_like(sse)\n",
    "    return float(r2.max()), int(np.argmax(r2))\n",
    "\n",
    "print(\"Building probe dataset...\")\n",
    "X_train, X_test, y_train, y_test = build_probe_dataset(model, char2int, device, Path(EVAL_FR_PATH), Path(EVAL_EN_PATH), CONFIG, norm)\n",
    "print(f\"Probe: {X_train.shape[0]} train, {X_test.shape[0]} test\")\n",
    "\n",
    "r2_local, best_neuron = ridge_r2_local(X_train, y_train, X_test, y_test, CONFIG[\"probe_alpha\"])\n",
    "print(f\"\\n{'='*50}\\nRESULTS\\n{'='*50}\")\n",
    "print(f\"Best R² (local): {r2_local:.4f}\")\n",
    "print(f\"Best neuron index: {best_neuron}\\n{'='*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u1oz33nAoANb",
    "outputId": "5769f4c5-eaf6-4a4e-b233-114a4a100622"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: best_lstm_model.pt\n",
      "  Vocab size: 115\n",
      "  R² score: 0.7363\n",
      "  Best neuron: 250\n"
     ]
    }
   ],
   "source": [
    "# Saving\n",
    "\n",
    "checkpoint = {\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"char2int\": char2int,\n",
    "    \"config\": CONFIG,\n",
    "    \"vocab_size\": len(char2int),\n",
    "    \"r2_local\": r2_local,\n",
    "    \"best_neuron\": best_neuron,\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, \"best_lstm_model.pt\")\n",
    "print(\"Saved: best_lstm_model.pt\")\n",
    "print(f\"  Vocab size: {len(char2int)}\")\n",
    "print(f\"  R² score: {r2_local:.4f}\")\n",
    "print(f\"  Best neuron: {best_neuron}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.14.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
