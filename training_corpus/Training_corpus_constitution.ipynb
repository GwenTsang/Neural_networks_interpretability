{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://raw.githubusercontent.com/GwenTsang/tests/refs/heads/main/all_Flaubert.txt\n",
        "!wget -q https://raw.githubusercontent.com/GwenTsang/tests/refs/heads/main/french.txt\n",
        "!wget -q https://raw.githubusercontent.com/GwenTsang/tests/refs/heads/main/l.txt"
      ],
      "metadata": {
        "id": "Ty66jb8PnJjU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import unicodedata\n",
        "import numpy as np\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Sequence\n",
        "\n",
        "CONFIG = {\n",
        "    \"max_pair_dist\": 280,\n",
        "    \"max_depth\": 2,\n",
        "    \"n_paragraphs\": 3000,\n",
        "    \"lowercase\": False,\n",
        "    \"fold_diacritics\": True,\n",
        "    \"digit_map\": \"none\",\n",
        "    \"collapse_whitespace\": True,\n",
        "    \"seed\": 10,\n",
        "    \"TRAIN_FILES\": [\"all_Flaubert.txt\", \"french.txt\", \"l.txt\"]\n",
        "}\n",
        "\n",
        "# --- Data Structures and Helper Functions ---\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class ParagraphInfo:\n",
        "    text: str\n",
        "    has_pair: bool\n",
        "    balanced: bool\n",
        "    max_pair_dist: int\n",
        "    max_depth: int\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class TextNormConfig:\n",
        "    lowercase: bool\n",
        "    fold_diacritics: bool\n",
        "    digit_map: str\n",
        "    collapse_whitespace: bool\n",
        "\n",
        "def _analyze_parentheses(par: str) -> Tuple[bool, bool, int, int]:\n",
        "    \"\"\"\n",
        "    Exact reproduction of the analysis logic used in the training script.\n",
        "    \"\"\"\n",
        "    has_pair = \"(\" in par and \")\" in par\n",
        "    stack, max_dist, depth, max_depth, balanced = [], 0, 0, 0, True\n",
        "    for i, ch in enumerate(par):\n",
        "        if ch == \"(\":\n",
        "            stack.append(i)\n",
        "            depth += 1\n",
        "            max_depth = max(max_depth, depth)\n",
        "        elif ch == \")\":\n",
        "            if not stack:\n",
        "                balanced = False\n",
        "                break\n",
        "            max_dist = max(max_dist, i - stack.pop())\n",
        "            depth = max(0, depth - 1)\n",
        "    if stack:\n",
        "        balanced = False\n",
        "    return has_pair, balanced, max_dist, max_depth\n",
        "\n",
        "def load_paragraphs(files: Sequence[Path]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Reads files and splits by blank lines.\n",
        "    Crucial: uses 'errors=\"ignore\"' as per original code.\n",
        "    \"\"\"\n",
        "    paragraphs = []\n",
        "    for fp in files:\n",
        "        if not fp.exists():\n",
        "            print(f\"Warning: File {fp} not found. Skipping.\")\n",
        "            continue\n",
        "        txt = fp.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "        # Split by blank lines (\\n\\n)\n",
        "        paragraphs.extend([p.strip() for p in re.split(r\"\\n\\s*\\n\", txt) if p.strip()])\n",
        "    return paragraphs\n",
        "\n",
        "def build_paragraph_infos(files: Sequence[Path]) -> List[ParagraphInfo]:\n",
        "    return [ParagraphInfo(p, *_analyze_parentheses(p)) for p in load_paragraphs(files)]\n",
        "\n",
        "def normalize_text(s: str, cfg: TextNormConfig) -> str:\n",
        "    \"\"\"\n",
        "    Applies text normalization exactly as the LSTM preprocessing did.\n",
        "    \"\"\"\n",
        "    if cfg.lowercase:\n",
        "        s = s.lower()\n",
        "    if cfg.fold_diacritics:\n",
        "        s = unicodedata.normalize(\"NFKD\", s)\n",
        "        s = \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
        "    if cfg.digit_map != \"none\":\n",
        "        s = re.sub(r\"\\d\", \"#\" if cfg.digit_map == \"hash\" else \"0\", s)\n",
        "    if cfg.collapse_whitespace:\n",
        "        s = re.sub(r\"[ \\t]+\", \" \", s)\n",
        "        s = re.sub(r\"\\n{3,}\", \"\\n\\n\", s)\n",
        "    return s\n",
        "\n",
        "def generate_corpus_content(infos: List[ParagraphInfo], config: dict) -> str:\n",
        "    \"\"\"\n",
        "    Filters, samples, and normalizes the text.\n",
        "    \"\"\"\n",
        "    # 1. Filter candidates\n",
        "    candidates = [\n",
        "        pi.text for pi in infos\n",
        "        if pi.has_pair\n",
        "        and pi.balanced\n",
        "        and pi.max_pair_dist <= config[\"max_pair_dist\"]\n",
        "        and pi.max_depth <= config[\"max_depth\"]\n",
        "    ]\n",
        "\n",
        "    print(f\"Total paragraphs found: {len(infos)}\")\n",
        "    print(f\"Candidates passing filter: {len(candidates)}\")\n",
        "\n",
        "    if not candidates:\n",
        "        return \"\"\n",
        "\n",
        "    # 2. Random Sampling\n",
        "    # Note: The original code creates a local generator inside the function\n",
        "    rng = np.random.default_rng(config[\"seed\"])\n",
        "\n",
        "    # Selection logic: rng.choice with replace=False\n",
        "    n_select = min(config[\"n_paragraphs\"], len(candidates))\n",
        "    selected_indices = rng.choice(len(candidates), size=n_select, replace=False)\n",
        "    chosen = [candidates[i] for i in selected_indices]\n",
        "\n",
        "    # 3. Normalization\n",
        "    norm_cfg = TextNormConfig(\n",
        "        config[\"lowercase\"],\n",
        "        config[\"fold_diacritics\"],\n",
        "        config[\"digit_map\"],\n",
        "        config[\"collapse_whitespace\"]\n",
        "    )\n",
        "\n",
        "    return \"\\n\\n\".join([normalize_text(p, norm_cfg) for p in chosen])\n",
        "\n",
        "# --- Main Execution ---\n",
        "\n",
        "def main():\n",
        "    input_files = [Path(f) for f in CONFIG[\"TRAIN_FILES\"]]\n",
        "    output_filename = \"reconstructed_lstm_corpus.txt\"\n",
        "\n",
        "    print(\"Analyzing input files...\")\n",
        "    infos = build_paragraph_infos(input_files)\n",
        "\n",
        "    print(\"Generating corpus text...\")\n",
        "    final_text = generate_corpus_content(infos, CONFIG)\n",
        "\n",
        "    print(f\"Writing {len(final_text):,} characters to {output_filename}...\")\n",
        "    with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(final_text)\n",
        "\n",
        "    print(\"Done.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJJRPunkZRmf",
        "outputId": "62b33d4e-2f34-4b5b-e144-ca1e243766a3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing input files...\n",
            "Generating corpus text...\n",
            "Total paragraphs found: 23644\n",
            "Candidates passing filter: 3249\n",
            "Writing 3,640,437 characters to reconstructed_lstm_corpus.txt...\n",
            "Done.\n"
          ]
        }
      ]
    }
  ]
}